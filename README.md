when matching 2 images in order to find the relative position of the robot with respect to a certain scene, we indeed need an image of reference over which we will perform the computation of the matching point. 
The problem in this case is that obviously the image of reference should be present somewhere, so that at each frame taken by the camera, the current image can be compared to the one of reference... 
But which is the one of reference? So, eiter we have a collection of images in the memory of the robot, and at each iteration we do the full matching between each current image and the reference,
or we use another network to classify the image to a scene (like we can find an element in the current frame that is also present in one of the reference image inside the collection), hence deciding the image of reference in the collection (hopefully with a shorter inference time), and then use superglue between the current frame and the selected image. 
Otherwise, we can try to scrape the web at each iteration... but I don't know how long it may take, nor if it's feasible to process so many images together...

...
It looks like the model is too slow to infer matching in real time... I think a solution may be to use, as stated, a convolutional neural network (something like yolo or whatever) in order to classify the current image frame into one of the image we have in the database, and do the matching just on those images
